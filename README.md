## NMT_GANT (SeqGAN + Negative Training)

SeqGAN, literally is a training methodology that applies GAN to the Sequence Generation model. It is difficult to apply GAN to the Sequence Generation model, but SeqGAN solves it through reinforcement learning techniques. And Negative Training is a training methodology that uses the data generated by the model again in the training process. The two aforementioned training methodologies have in common that they are designed to improve the model's  generation ability, and that they utilize sentence generation in the training process. Learning through actual sentence generation will be of great help to improve the model's generation ability, but training the model with various and many dataset seems better in terms of efficiency. Therefore, in the repo, we conduct experiments to further improve the performance of the model while simultaneously applying the two techniques in one generation.

<br>
<br>

## Model

**BaseLine** <br>
> Encoder-Decoder Model (yet to fixed)

<br>

**SeqGAN** <br>
> Apply SeqGAN only

<br>

**Negative Training** <br>
> Apply Negative Training only

<br>

**GANT(SeqGAN + Negative Training)** <br>
> Apply SeqGAN and Negative Training at the same time.

<br>
<br>

## Configurations

<br>
<br>

## Result

<br>
<br>

## Reference

<br>
